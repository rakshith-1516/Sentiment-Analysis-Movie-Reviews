{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c2e022d-1461-409e-a77a-2f240fb5df8b",
   "metadata": {
    "id": "4c2e022d-1461-409e-a77a-2f240fb5df8b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v0/93wqvg812jx_00d7jl14lxfw0000gn/T/ipykernel_11649/3393215015.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "041b4f34-095d-4b01-8e5e-b72c0ccb78ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 8.854186058044434 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>thumbsup</th>\n",
       "      <th>thumbsdown</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B005LAIHE0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>rented expecting somewhat cheesy rom com under...</td>\n",
       "      <td>better expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B002GTZSZU</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>always loved classical music movie made stick ...</td>\n",
       "      <td>great movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0004Z3558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>good investment u husband really love karate h...</td>\n",
       "      <td>cute kid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00004U3ZU</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>mario bava founding father italian horror see ...</td>\n",
       "      <td>landmark horror movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B001B73PO4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>entire series watched year old winter school b...</td>\n",
       "      <td>liberty kid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    productId  thumbsup  thumbsdown  sentiment  \\\n",
       "0  B005LAIHE0         1           0          1   \n",
       "1  B002GTZSZU         2           1          1   \n",
       "2  B0004Z3558         1           0          1   \n",
       "3  B00004U3ZU        25           0          1   \n",
       "4  B001B73PO4         1           2          1   \n",
       "\n",
       "                                   preprocessed_text   preprocessed_summary  \n",
       "0  rented expecting somewhat cheesy rom com under...        better expected  \n",
       "1  always loved classical music movie made stick ...            great movie  \n",
       "2  good investment u husband really love karate h...               cute kid  \n",
       "3  mario bava founding father italian horror see ...  landmark horror movie  \n",
       "4  entire series watched year old winter school b...            liberty kid  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df = pd.read_csv('preprocessed_data.csv')\n",
    "end_time = time.time()\n",
    "print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3cbffe1-df22-4559-a02d-24de6a62aaa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3cbffe1-df22-4559-a02d-24de6a62aaa9",
    "outputId": "1b5d80ea-f2f2-4d32-ffba-7b467673a5ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "productId               1999965\n",
       "thumbsup                1999965\n",
       "thumbsdown              1999965\n",
       "sentiment               1999965\n",
       "preprocessed_text       1999953\n",
       "preprocessed_summary    1980254\n",
       "total_text              1980243\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['total_text'] = df['preprocessed_summary']+' '+df['preprocessed_text']\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f20b2a-0c97-4ae5-8063-14b01072aa75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "49f20b2a-0c97-4ae5-8063-14b01072aa75",
    "outputId": "2ed3c653-a241-4774-bc5d-037017913bb4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "productId               1980243\n",
       "thumbsup                1980243\n",
       "thumbsdown              1980243\n",
       "sentiment               1980243\n",
       "preprocessed_text       1980243\n",
       "preprocessed_summary    1980243\n",
       "total_text              1980243\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(how='any')\n",
    "df = df.reset_index(drop=True)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fefdde1-2902-4bf4-b604-0013520fc99a",
   "metadata": {
    "id": "8fefdde1-2902-4bf4-b604-0013520fc99a"
   },
   "outputs": [],
   "source": [
    "# Get a random sample of 100 rows with sentiment equal to Positive\n",
    "random_sample_sentiment_pos = df[df['sentiment'] == 1].sample(n=50000, random_state=42)\n",
    "\n",
    "# Get a random sample of 100 rows with sentiment equal to Negative\n",
    "random_sample_sentiment_neg = df[df['sentiment'] == -1].sample(n=50000, random_state=42)\n",
    "\n",
    "random_samples_combined = pd.concat([random_sample_sentiment_pos, random_sample_sentiment_neg])\n",
    "\n",
    "X = random_samples_combined.drop(columns=['sentiment'], axis=1)\n",
    "y = random_samples_combined['sentiment']\n",
    "\n",
    "y = y.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bded6cf-28f1-4358-b38a-62cdd75019e0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 736
    },
    "id": "2bded6cf-28f1-4358-b38a-62cdd75019e0",
    "outputId": "16ccdd23-453a-4e03-ca98-bcbd500818ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>thumbsup</th>\n",
       "      <th>thumbsdown</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>preprocessed_summary</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260745</th>\n",
       "      <td>B00004VXTF</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>wanted upgrade vhs dvd also got share movie so...</td>\n",
       "      <td>pulp</td>\n",
       "      <td>pulp wanted upgrade vhs dvd also got share mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397160</th>\n",
       "      <td>B00005J4A7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>good movie every angle good winning evil age o...</td>\n",
       "      <td>great entertainer</td>\n",
       "      <td>great entertainer good movie every angle good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>B0051GOB26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>got boyfriend christmas loved shipped really f...</td>\n",
       "      <td>fast shipping great movie</td>\n",
       "      <td>fast shipping great movie got boyfriend christ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184318</th>\n",
       "      <td>B000059ZAT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>one best classic film good acting good combina...</td>\n",
       "      <td>spartacus</td>\n",
       "      <td>spartacus one best classic film good acting go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802638</th>\n",
       "      <td>B000WULC0K</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>vci entertainment present christmas carol aka ...</td>\n",
       "      <td>christmas carol alastair sim vci raised bar on...</td>\n",
       "      <td>christmas carol alastair sim vci raised bar on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413560</th>\n",
       "      <td>B000JCSPG0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>well finished watching director cut last night...</td>\n",
       "      <td>quot director cut quot</td>\n",
       "      <td>quot director cut quot well finished watching ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1185477</th>\n",
       "      <td>B001H1SW4M</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>creepy movie sure creepy enough film begin pro...</td>\n",
       "      <td>ridiculous</td>\n",
       "      <td>ridiculous creepy movie sure creepy enough fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1343585</th>\n",
       "      <td>B00144N8MI</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>generally good photography marred unfocused ed...</td>\n",
       "      <td>overvalued</td>\n",
       "      <td>overvalued generally good photography marred u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336372</th>\n",
       "      <td>B0009QZ48I</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>read review dvd went ahead bought anyway live ...</td>\n",
       "      <td>listened review</td>\n",
       "      <td>listened review read review dvd went ahead bou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1912081</th>\n",
       "      <td>B006TTC5QS</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>well first half movie redone least left editin...</td>\n",
       "      <td>oh</td>\n",
       "      <td>oh well first half movie redone least left edi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          productId  thumbsup  thumbsdown  \\\n",
       "260745   B00004VXTF         0           1   \n",
       "397160   B00005J4A7         2           2   \n",
       "2725     B0051GOB26         0           1   \n",
       "184318   B000059ZAT         2           2   \n",
       "802638   B000WULC0K         5           1   \n",
       "...             ...       ...         ...   \n",
       "1413560  B000JCSPG0         0           5   \n",
       "1185477  B001H1SW4M        18          10   \n",
       "1343585  B00144N8MI         6           2   \n",
       "1336372  B0009QZ48I         2           0   \n",
       "1912081  B006TTC5QS         6          21   \n",
       "\n",
       "                                         preprocessed_text  \\\n",
       "260745   wanted upgrade vhs dvd also got share movie so...   \n",
       "397160   good movie every angle good winning evil age o...   \n",
       "2725     got boyfriend christmas loved shipped really f...   \n",
       "184318   one best classic film good acting good combina...   \n",
       "802638   vci entertainment present christmas carol aka ...   \n",
       "...                                                    ...   \n",
       "1413560  well finished watching director cut last night...   \n",
       "1185477  creepy movie sure creepy enough film begin pro...   \n",
       "1343585  generally good photography marred unfocused ed...   \n",
       "1336372  read review dvd went ahead bought anyway live ...   \n",
       "1912081  well first half movie redone least left editin...   \n",
       "\n",
       "                                      preprocessed_summary  \\\n",
       "260745                                                pulp   \n",
       "397160                                   great entertainer   \n",
       "2725                             fast shipping great movie   \n",
       "184318                                           spartacus   \n",
       "802638   christmas carol alastair sim vci raised bar on...   \n",
       "...                                                    ...   \n",
       "1413560                             quot director cut quot   \n",
       "1185477                                         ridiculous   \n",
       "1343585                                         overvalued   \n",
       "1336372                                    listened review   \n",
       "1912081                                                 oh   \n",
       "\n",
       "                                                total_text  \n",
       "260745   pulp wanted upgrade vhs dvd also got share mov...  \n",
       "397160   great entertainer good movie every angle good ...  \n",
       "2725     fast shipping great movie got boyfriend christ...  \n",
       "184318   spartacus one best classic film good acting go...  \n",
       "802638   christmas carol alastair sim vci raised bar on...  \n",
       "...                                                    ...  \n",
       "1413560  quot director cut quot well finished watching ...  \n",
       "1185477  ridiculous creepy movie sure creepy enough fil...  \n",
       "1343585  overvalued generally good photography marred u...  \n",
       "1336372  listened review read review dvd went ahead bou...  \n",
       "1912081  oh well first half movie redone least left edi...  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90de5912-6773-48cc-a182-d7298affc637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 6) (60000,)\n",
      "Validation set shape: (20000, 6) (20000,)\n",
      "Testing set shape: (20000, 6) (20000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data into training, validation and testing sets (60%, 20%, 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "print(\"Testing set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2f087c4-1ddb-41fb-a9ff-ceccf41a9701",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2f087c4-1ddb-41fb-a9ff-ceccf41a9701",
    "outputId": "70faec7b-d25d-4a4b-dc83-d7c9aec593cf"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def getTfidfVectorizerFeatures(max_features):\n",
    "  vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "  X_train_vectorized = vectorizer.fit_transform(X_train['total_text'])\n",
    "  X_val_vectorized = vectorizer.transform(X_val['total_text'])\n",
    "  X_test_vectorized = vectorizer.transform(X_test['total_text'])\n",
    "  scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "  X_train_scaled = scaler.fit_transform(X_train_vectorized)\n",
    "  X_val_scaled = scaler.transform(X_val_vectorized)\n",
    "  X_test_scaled = scaler.transform(X_test_vectorized)\n",
    "  return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01cc013a-25a7-4756-a074-238967bc95c7",
   "metadata": {
    "id": "01cc013a-25a7-4756-a074-238967bc95c7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def getCountVectorizerFeatures(max_features):\n",
    "  vectorizer = CountVectorizer(max_features=max_features)\n",
    "  X_train_vectorized = vectorizer.fit_transform(X_train['total_text'])\n",
    "  X_val_vectorized = vectorizer.transform(X_val['total_text'])\n",
    "  X_test_vectorized = vectorizer.transform(X_test['total_text'])\n",
    "  scaler = preprocessing.StandardScaler(with_mean=False)\n",
    "  X_train_scaled = scaler.fit_transform(X_train_vectorized)\n",
    "  X_val_scaled = scaler.transform(X_val_vectorized)\n",
    "  X_test_scaled = scaler.transform(X_test_vectorized)\n",
    "  return X_train_scaled, X_val_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a1bddb0-19c9-4d9b-8a84-c1ac0a3b5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "def getGloveEmbeddings(max_features):\n",
    "    glove_file = '../HW2/glove.6B/'+max_features\n",
    "    glove_model = load_glove_embeddings(glove_file)\n",
    "\n",
    "    X_train_vectorized = []\n",
    "    for text in X_train['total_text']:\n",
    "        words = text.split()[:5000]\n",
    "        # mean embedding\n",
    "        embedding = np.mean([glove_model[word] for word in words if word in glove_model], axis=0)\n",
    "        X_train_vectorized.append(embedding)\n",
    "\n",
    "    X_val_vectorized = []\n",
    "    for text in X_val['total_text']:\n",
    "        words = text.split()[:5000]\n",
    "        # mean embedding\n",
    "        embedding = np.mean([glove_model[word] for word in words if word in glove_model], axis=0)\n",
    "        X_val_vectorized.append(embedding)\n",
    "\n",
    "    X_test_vectorized = []\n",
    "    for text in X_test['total_text']:\n",
    "        words = text.split()[:5000]\n",
    "        # mean embedding\n",
    "        embedding = np.mean([glove_model[word] for word in words if word in glove_model], axis=0)\n",
    "        X_test_vectorized.append(embedding)\n",
    "\n",
    "    return X_train_vectorized, X_val_vectorized, X_test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847fc631-a3d6-46ce-8bf9-829e7a31867a",
   "metadata": {
    "id": "847fc631-a3d6-46ce-8bf9-829e7a31867a"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def LogisticRegressionModel(X_train_scaled, X_val_scaled):\n",
    "  start_time = time.time()\n",
    "  lr_cv_model = LogisticRegression(penalty='l2',max_iter=1000,C=1,random_state=42)\n",
    "  lr_cv_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "  predicted_train = lr_cv_model.predict(X_train_scaled)\n",
    "  predicted_val = lr_cv_model.predict(X_val_scaled)\n",
    "\n",
    "  accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "  accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "  print(\"Training Accuracy: \", accuracy_train)\n",
    "  print(\"Validation Accuracy: \", accuracy_val)\n",
    "  lr_cv_report=classification_report(y_val,predicted_val,target_names=['Positive','Negative'])\n",
    "  print(\"Classification Report\")\n",
    "  print(lr_cv_report)\n",
    "  lr_cv_cm = confusion_matrix(y_val, predicted_val)\n",
    "  print(\"Confusion Matrix\")\n",
    "  print(lr_cv_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6beba45f-91b7-404c-8304-abfabcb37ef4",
   "metadata": {
    "id": "6beba45f-91b7-404c-8304-abfabcb37ef4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def SVMClassifier(X_train_scaled, X_val_scaled):\n",
    "  start_time = time.time()\n",
    "  svm_cv_model = SGDClassifier(loss='hinge',max_iter=1000,random_state=42)\n",
    "  svm_cv_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "  predicted_train = svm_cv_model.predict(X_train_scaled)\n",
    "  predicted_val = svm_cv_model.predict(X_val_scaled)\n",
    "\n",
    "  accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "  accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "  print(\"Training Accuracy: \", accuracy_train)\n",
    "  print(\"Validation Accuracy: \", accuracy_val)\n",
    "  svm_cv_report=classification_report(y_val,predicted_val,target_names=['Positive','Negative'])\n",
    "  print(\"Classification Report\")\n",
    "  print(svm_cv_report)\n",
    "  svm_cv_cm = confusion_matrix(y_val, predicted_val)\n",
    "  print(\"Confusion Matrix\")\n",
    "  print(svm_cv_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3997af0f-141e-4fda-867f-6f73ef8d2a35",
   "metadata": {
    "id": "3997af0f-141e-4fda-867f-6f73ef8d2a35"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def MNBClassifier(X_train_scaled, X_val_scaled):\n",
    "  start_time = time.time()\n",
    "  mnb_cv_model = MultinomialNB()\n",
    "  mnb_cv_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "  predicted_train = mnb_cv_model.predict(X_train_scaled)\n",
    "  predicted_val = mnb_cv_model.predict(X_val_scaled)\n",
    "\n",
    "  accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "  accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "  print(\"Training Accuracy: \", accuracy_train)\n",
    "  print(\"Validation Accuracy: \", accuracy_val)\n",
    "  mnb_cv_report=classification_report(y_val,predicted_val,target_names=['Positive','Negative'])\n",
    "  print(\"Classification Report\")\n",
    "  print(mnb_cv_report)\n",
    "  mnb_cv_cm = confusion_matrix(y_val, predicted_val)\n",
    "  print(\"Confusion Matrix\")\n",
    "  print(mnb_cv_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff8ed82c-0b91-41f7-a001-435ebe4dc007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def GNBClassifier(X_train_scaled, X_val_scaled):\n",
    "  start_time = time.time()\n",
    "  gnb_cv_model = GaussianNB()\n",
    "  gnb_cv_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "  predicted_train = gnb_cv_model.predict(X_train_scaled)\n",
    "  predicted_val = gnb_cv_model.predict(X_val_scaled)\n",
    "\n",
    "  accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "  accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "  print(\"Training Accuracy: \", accuracy_train)\n",
    "  print(\"Validation Accuracy: \", accuracy_val)\n",
    "  gnb_cv_report=classification_report(y_val,predicted_val,target_names=['Positive','Negative'])\n",
    "  print(\"Classification Report\")\n",
    "  print(gnb_cv_report)\n",
    "  gnb_cv_cm = confusion_matrix(y_val, predicted_val)\n",
    "  print(\"Confusion Matrix\")\n",
    "  print(gnb_cv_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e011e6b-c119-49c5-9ed6-71515f061a40",
   "metadata": {
    "id": "1e011e6b-c119-49c5-9ed6-71515f061a40"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "\n",
    "def MLPClassifier(X_train_scaled, X_val_scaled):\n",
    "  start_time = time.time()\n",
    "\n",
    "  if isinstance(X_train_scaled, list):\n",
    "      shape = len(X_train_scaled[1])\n",
    "      X_train_scaled = np.array(X_train_scaled)\n",
    "      X_val_scaled = np.array(X_val_scaled)\n",
    "  else:\n",
    "      shape = X_train_scaled.shape[1]\n",
    "      X_train_scaled = X_train_scaled.toarray()\n",
    "      X_val_scaled = X_val_scaled.toarray()\n",
    "      \n",
    "  def getModel():\n",
    "      mlp_classifier = Sequential([\n",
    "      Dense(128, activation='relu', input_shape=(shape,)),\n",
    "      Dense(128, activation='relu'),\n",
    "      Dense(1, activation='sigmoid')\n",
    "      ])\n",
    "      optimizer = Adam()\n",
    "      mlp_classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "      return mlp_classifier\n",
    "\n",
    "  mlp_model = getModel()\n",
    "  mlp_model.fit(X_train_scaled, y_train, epochs=20, batch_size=100, verbose=False)\n",
    "\n",
    "  predicted_train = mlp_model.predict(X_train_scaled)\n",
    "  predicted_val = mlp_model.predict(X_val_scaled)\n",
    "\n",
    "  predicted_train = (predicted_train > 0.5).astype(int)\n",
    "  predicted_val = (predicted_val > 0.5).astype(int)\n",
    "\n",
    "  accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "  accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "  end_time = time.time()\n",
    "  print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "  print(\"Training Accuracy: \", accuracy_train)\n",
    "  print(\"Validation Accuracy: \", accuracy_val)\n",
    "  mlp_cv_report=classification_report(y_val,predicted_val,target_names=['Positive','Negative'])\n",
    "  print(\"Classification Report\")\n",
    "  print(mlp_cv_report)\n",
    "  mlp_cv_cm = confusion_matrix(y_val, predicted_val)\n",
    "  print(\"Confusion Matrix\")\n",
    "  print(mlp_cv_cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86123659-5bf3-4254-9aaa-b7bee85ec842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Embeddings No. of Dimensions =  glove.6B.50d.txt\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 7.723412036895752 seconds\n",
      "Training Accuracy:  0.7653433333333334\n",
      "Validation Accuracy:  0.76504\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.78      0.77    100000\n",
      "    Negative       0.77      0.75      0.76    100000\n",
      "\n",
      "    accuracy                           0.77    200000\n",
      "   macro avg       0.77      0.77      0.77    200000\n",
      "weighted avg       0.77      0.77      0.77    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[77552 22448]\n",
      " [24544 75456]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 1.6705729961395264 seconds\n",
      "Training Accuracy:  0.7621466666666666\n",
      "Validation Accuracy:  0.76217\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.74      0.80      0.77    100000\n",
      "    Negative       0.78      0.72      0.75    100000\n",
      "\n",
      "    accuracy                           0.76    200000\n",
      "   macro avg       0.76      0.76      0.76    200000\n",
      "weighted avg       0.76      0.76      0.76    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[80039 19961]\n",
      " [27605 72395]]\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "Time taken: 0.7668290138244629 seconds\n",
      "Training Accuracy:  0.7126433333333333\n",
      "Validation Accuracy:  0.713275\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.68      0.79      0.73    100000\n",
      "    Negative       0.75      0.63      0.69    100000\n",
      "\n",
      "    accuracy                           0.71    200000\n",
      "   macro avg       0.72      0.71      0.71    200000\n",
      "weighted avg       0.72      0.71      0.71    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[79205 20795]\n",
      " [36550 63450]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 236us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298us/step\n",
      "Time taken: 69.39710092544556 seconds\n",
      "Training Accuracy:  0.8219116666666667\n",
      "Validation Accuracy:  0.807765\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.80      0.82      0.81    100000\n",
      "    Negative       0.82      0.79      0.80    100000\n",
      "\n",
      "    accuracy                           0.81    200000\n",
      "   macro avg       0.81      0.81      0.81    200000\n",
      "weighted avg       0.81      0.81      0.81    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[82312 17688]\n",
      " [20759 79241]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Glove Embeddings No. of Dimensions =  glove.6B.100d.txt\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 7.420844078063965 seconds\n",
      "Training Accuracy:  0.8053933333333333\n",
      "Validation Accuracy:  0.80515\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.80      0.81      0.81    100000\n",
      "    Negative       0.81      0.80      0.80    100000\n",
      "\n",
      "    accuracy                           0.81    200000\n",
      "   macro avg       0.81      0.81      0.81    200000\n",
      "weighted avg       0.81      0.81      0.81    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[80953 19047]\n",
      " [19923 80077]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 2.2685439586639404 seconds\n",
      "Training Accuracy:  0.8049766666666667\n",
      "Validation Accuracy:  0.80499\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.80      0.82      0.81    100000\n",
      "    Negative       0.81      0.79      0.80    100000\n",
      "\n",
      "    accuracy                           0.80    200000\n",
      "   macro avg       0.81      0.80      0.80    200000\n",
      "weighted avg       0.81      0.80      0.80    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[81524 18476]\n",
      " [20526 79474]]\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "Time taken: 1.3457527160644531 seconds\n",
      "Training Accuracy:  0.7079966666666667\n",
      "Validation Accuracy:  0.707025\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.68      0.79      0.73    100000\n",
      "    Negative       0.75      0.62      0.68    100000\n",
      "\n",
      "    accuracy                           0.71    200000\n",
      "   macro avg       0.71      0.71      0.70    200000\n",
      "weighted avg       0.71      0.71      0.70    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[79478 20522]\n",
      " [38073 61927]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 243us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 240us/step\n",
      "Time taken: 77.29339098930359 seconds\n",
      "Training Accuracy:  0.865705\n",
      "Validation Accuracy:  0.8486\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.87      0.85    100000\n",
      "    Negative       0.87      0.83      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87175 12825]\n",
      " [17455 82545]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Glove Embeddings No. of Dimensions =  glove.6B.200d.txt\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 20.349468231201172 seconds\n",
      "Training Accuracy:  0.832105\n",
      "Validation Accuracy:  0.83128\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.83      0.83    100000\n",
      "    Negative       0.83      0.83      0.83    100000\n",
      "\n",
      "    accuracy                           0.83    200000\n",
      "   macro avg       0.83      0.83      0.83    200000\n",
      "weighted avg       0.83      0.83      0.83    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[83250 16750]\n",
      " [16994 83006]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 4.324131727218628 seconds\n",
      "Training Accuracy:  0.8304183333333334\n",
      "Validation Accuracy:  0.82993\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.83      0.83    100000\n",
      "    Negative       0.83      0.83      0.83    100000\n",
      "\n",
      "    accuracy                           0.83    200000\n",
      "   macro avg       0.83      0.83      0.83    200000\n",
      "weighted avg       0.83      0.83      0.83    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[82910 17090]\n",
      " [16924 83076]]\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "Time taken: 3.17958402633667 seconds\n",
      "Training Accuracy:  0.70855\n",
      "Validation Accuracy:  0.70845\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.67      0.81      0.73    100000\n",
      "    Negative       0.76      0.61      0.68    100000\n",
      "\n",
      "    accuracy                           0.71    200000\n",
      "   macro avg       0.72      0.71      0.71    200000\n",
      "weighted avg       0.72      0.71      0.71    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[80585 19415]\n",
      " [38895 61105]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 278us/step\n",
      "Time taken: 81.45496010780334 seconds\n",
      "Training Accuracy:  0.897765\n",
      "Validation Accuracy:  0.87596\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.87      0.88      0.88    100000\n",
      "    Negative       0.88      0.87      0.87    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[88424 11576]\n",
      " [13232 86768]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Glove Embeddings No. of Dimensions =  glove.6B.300d.txt\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 30.151658058166504 seconds\n",
      "Training Accuracy:  0.841995\n",
      "Validation Accuracy:  0.841565\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.84      0.84      0.84    100000\n",
      "    Negative       0.84      0.84      0.84    100000\n",
      "\n",
      "    accuracy                           0.84    200000\n",
      "   macro avg       0.84      0.84      0.84    200000\n",
      "weighted avg       0.84      0.84      0.84    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[84346 15654]\n",
      " [16033 83967]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 7.030525207519531 seconds\n",
      "Training Accuracy:  0.8410816666666666\n",
      "Validation Accuracy:  0.84024\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.85      0.84    100000\n",
      "    Negative       0.85      0.83      0.84    100000\n",
      "\n",
      "    accuracy                           0.84    200000\n",
      "   macro avg       0.84      0.84      0.84    200000\n",
      "weighted avg       0.84      0.84      0.84    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[85421 14579]\n",
      " [17373 82627]]\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "Time taken: 5.6524529457092285 seconds\n",
      "Training Accuracy:  0.7075633333333333\n",
      "Validation Accuracy:  0.708455\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.67      0.81      0.74    100000\n",
      "    Negative       0.76      0.61      0.68    100000\n",
      "\n",
      "    accuracy                           0.71    200000\n",
      "   macro avg       0.72      0.71      0.71    200000\n",
      "weighted avg       0.72      0.71      0.71    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[80931 19069]\n",
      " [39240 60760]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 270us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 275us/step\n",
      "Time taken: 89.64800500869751 seconds\n",
      "Training Accuracy:  0.9153083333333333\n",
      "Validation Accuracy:  0.886465\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.89      0.89    100000\n",
      "    Negative       0.89      0.88      0.89    100000\n",
      "\n",
      "    accuracy                           0.89    200000\n",
      "   macro avg       0.89      0.89      0.89    200000\n",
      "weighted avg       0.89      0.89      0.89    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[89292 10708]\n",
      " [11999 88001]]\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_feature_sizes = ['glove.6B.50d.txt', 'glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt']\n",
    "for size in max_feature_sizes:\n",
    "  print(\"Glove Embeddings No. of Dimensions = \", size)\n",
    "  print(\"\")\n",
    "  X_train_scaled, X_val_scaled, X_test_scaled = getGloveEmbeddings(size)\n",
    "  print(\"Logistic Regression\")\n",
    "  print(\"\")\n",
    "  LogisticRegressionModel(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Support Vector Machine\")\n",
    "  print(\"\")\n",
    "  SVMClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Gaussian Naive Bayes\")\n",
    "  print(\"\")\n",
    "  GNBClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Multilayer Perceptron\")\n",
    "  print(\"\")\n",
    "  MLPClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69bf097f-6534-4190-b58b-9b9170d28892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf Vectorizer No. of Features =  500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 1.2804358005523682 seconds\n",
      "Training Accuracy:  0.849875\n",
      "Validation Accuracy:  0.8492\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.84      0.85    100000\n",
      "    Negative       0.84      0.86      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[84289 15711]\n",
      " [14449 85551]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 4.001117944717407 seconds\n",
      "Training Accuracy:  0.8408816666666666\n",
      "Validation Accuracy:  0.839735\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.83      0.84    100000\n",
      "    Negative       0.83      0.85      0.84    100000\n",
      "\n",
      "    accuracy                           0.84    200000\n",
      "   macro avg       0.84      0.84      0.84    200000\n",
      "weighted avg       0.84      0.84      0.84    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[83127 16873]\n",
      " [15180 84820]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.223052978515625 seconds\n",
      "Training Accuracy:  0.8268166666666666\n",
      "Validation Accuracy:  0.82602\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.82      0.82    100000\n",
      "    Negative       0.82      0.83      0.83    100000\n",
      "\n",
      "    accuracy                           0.83    200000\n",
      "   macro avg       0.83      0.83      0.83    200000\n",
      "weighted avg       0.83      0.83      0.83    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[82014 17986]\n",
      " [16810 83190]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 315us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298us/step\n",
      "Time taken: 121.2576949596405 seconds\n",
      "Training Accuracy:  0.975595\n",
      "Validation Accuracy:  0.906255\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.92      0.91    100000\n",
      "    Negative       0.92      0.89      0.90    100000\n",
      "\n",
      "    accuracy                           0.91    200000\n",
      "   macro avg       0.91      0.91      0.91    200000\n",
      "weighted avg       0.91      0.91      0.91    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[92455  7545]\n",
      " [11204 88796]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Tfidf Vectorizer No. of Features =  1500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 1.884268045425415 seconds\n",
      "Training Accuracy:  0.8869383333333334\n",
      "Validation Accuracy:  0.884345\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.88      0.88    100000\n",
      "    Negative       0.88      0.89      0.89    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87758 12242]\n",
      " [10889 89111]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 7.195940017700195 seconds\n",
      "Training Accuracy:  0.8777183333333334\n",
      "Validation Accuracy:  0.876245\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.88      0.88    100000\n",
      "    Negative       0.88      0.88      0.88    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87531 12469]\n",
      " [12282 87718]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.2762181758880615 seconds\n",
      "Training Accuracy:  0.8493216666666666\n",
      "Validation Accuracy:  0.849375\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.84      0.85    100000\n",
      "    Negative       0.85      0.85      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[84443 15557]\n",
      " [14568 85432]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 361us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 357us/step\n",
      "Time taken: 221.21512126922607 seconds\n",
      "Training Accuracy:  0.9974716666666666\n",
      "Validation Accuracy:  0.93577\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.92      0.95      0.94    100000\n",
      "    Negative       0.95      0.92      0.93    100000\n",
      "\n",
      "    accuracy                           0.94    200000\n",
      "   macro avg       0.94      0.94      0.94    200000\n",
      "weighted avg       0.94      0.94      0.94    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[95110  4890]\n",
      " [ 7956 92044]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Tfidf Vectorizer No. of Features =  2500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 2.0013020038604736 seconds\n",
      "Training Accuracy:  0.89761\n",
      "Validation Accuracy:  0.894305\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.90      0.89      0.89    100000\n",
      "    Negative       0.89      0.90      0.89    100000\n",
      "\n",
      "    accuracy                           0.89    200000\n",
      "   macro avg       0.89      0.89      0.89    200000\n",
      "weighted avg       0.89      0.89      0.89    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[88861 11139]\n",
      " [10000 90000]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 8.262901067733765 seconds\n",
      "Training Accuracy:  0.8859783333333333\n",
      "Validation Accuracy:  0.883025\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.88      0.88    100000\n",
      "    Negative       0.88      0.89      0.88    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87997 12003]\n",
      " [11392 88608]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.2929861545562744 seconds\n",
      "Training Accuracy:  0.8510566666666667\n",
      "Validation Accuracy:  0.850665\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.85      0.85    100000\n",
      "    Negative       0.85      0.85      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[84901 15099]\n",
      " [14768 85232]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakshithc/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/layers/core/dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18750/18750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 447us/step\n",
      "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 447us/step\n",
      "Time taken: 331.0425090789795 seconds\n",
      "Training Accuracy:  0.9987483333333333\n",
      "Validation Accuracy:  0.942755\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.93      0.95      0.94    100000\n",
      "    Negative       0.95      0.93      0.94    100000\n",
      "\n",
      "    accuracy                           0.94    200000\n",
      "   macro avg       0.94      0.94      0.94    200000\n",
      "weighted avg       0.94      0.94      0.94    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[95234  4766]\n",
      " [ 6683 93317]]\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_feature_sizes = [500, 1500, 2500]\n",
    "for size in max_feature_sizes:\n",
    "  print(\"Tfidf Vectorizer No. of Features = \", size)\n",
    "  print(\"\")\n",
    "  X_train_scaled, X_val_scaled, X_test_scaled = getTfidfVectorizerFeatures(size)\n",
    "  print(\"Logistic Regression\")\n",
    "  print(\"\")\n",
    "  LogisticRegressionModel(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Support Vector Machine\")\n",
    "  print(\"\")\n",
    "  SVMClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Multinomial Naive Bayes\")\n",
    "  print(\"\")\n",
    "  MNBClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Multilayer Perceptron\")\n",
    "  print(\"\")\n",
    "  MLPClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4L39uFU3T9Xc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4L39uFU3T9Xc",
    "outputId": "381db944-1ec9-40fc-fa45-3b891ecb274a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Vectorizer No. of Features =  500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 2.277576446533203 seconds\n",
      "Training Accuracy:  0.8483383333333333\n",
      "Validation Accuracy:  0.84742\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.86      0.83      0.85    100000\n",
      "    Negative       0.84      0.86      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[83396 16604]\n",
      " [13912 86088]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 7.507729530334473 seconds\n",
      "Training Accuracy:  0.842605\n",
      "Validation Accuracy:  0.84116\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.86      0.82      0.84    100000\n",
      "    Negative       0.82      0.87      0.85    100000\n",
      "\n",
      "    accuracy                           0.84    200000\n",
      "   macro avg       0.84      0.84      0.84    200000\n",
      "weighted avg       0.84      0.84      0.84    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[81504 18496]\n",
      " [13272 86728]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.32501959800720215 seconds\n",
      "Training Accuracy:  0.82592\n",
      "Validation Accuracy:  0.825295\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.81      0.82    100000\n",
      "    Negative       0.82      0.84      0.83    100000\n",
      "\n",
      "    accuracy                           0.83    200000\n",
      "   macro avg       0.83      0.83      0.83    200000\n",
      "weighted avg       0.83      0.83      0.83    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[81248 18752]\n",
      " [16189 83811]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n",
      "18750/18750 [==============================] - 27s 1ms/step\n",
      "6250/6250 [==============================] - 9s 1ms/step\n",
      "Time taken: 369.16979718208313 seconds\n",
      "Training Accuracy:  0.9723816666666667\n",
      "Validation Accuracy:  0.905785\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.90      0.92      0.91    100000\n",
      "    Negative       0.91      0.90      0.90    100000\n",
      "\n",
      "    accuracy                           0.91    200000\n",
      "   macro avg       0.91      0.91      0.91    200000\n",
      "weighted avg       0.91      0.91      0.91    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[91576  8424]\n",
      " [10419 89581]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Count Vectorizer No. of Features =  1500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 3.3171513080596924 seconds\n",
      "Training Accuracy:  0.8850316666666667\n",
      "Validation Accuracy:  0.88294\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.87      0.88    100000\n",
      "    Negative       0.87      0.90      0.88    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87013 12987]\n",
      " [10425 89575]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 10.959624767303467 seconds\n",
      "Training Accuracy:  0.8781316666666666\n",
      "Validation Accuracy:  0.876615\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.88      0.88    100000\n",
      "    Negative       0.88      0.88      0.88    100000\n",
      "\n",
      "    accuracy                           0.88    200000\n",
      "   macro avg       0.88      0.88      0.88    200000\n",
      "weighted avg       0.88      0.88      0.88    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[87795 12205]\n",
      " [12472 87528]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.4039943218231201 seconds\n",
      "Training Accuracy:  0.8481933333333334\n",
      "Validation Accuracy:  0.847345\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.85      0.85    100000\n",
      "    Negative       0.85      0.85      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[84909 15091]\n",
      " [15440 84560]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n",
      "18750/18750 [==============================] - 29s 2ms/step\n",
      "6250/6250 [==============================] - 10s 2ms/step\n",
      "Time taken: 485.36062717437744 seconds\n",
      "Training Accuracy:  0.9975416666666667\n",
      "Validation Accuracy:  0.936385\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.92      0.95      0.94    100000\n",
      "    Negative       0.95      0.92      0.94    100000\n",
      "\n",
      "    accuracy                           0.94    200000\n",
      "   macro avg       0.94      0.94      0.94    200000\n",
      "weighted avg       0.94      0.94      0.94    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[95126  4874]\n",
      " [ 7849 92151]]\n",
      "\n",
      "-----------------------------------------------------\n",
      "Count Vectorizer No. of Features =  2500\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Time taken: 4.473446846008301 seconds\n",
      "Training Accuracy:  0.89589\n",
      "Validation Accuracy:  0.892585\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.90      0.88      0.89    100000\n",
      "    Negative       0.88      0.90      0.89    100000\n",
      "\n",
      "    accuracy                           0.89    200000\n",
      "   macro avg       0.89      0.89      0.89    200000\n",
      "weighted avg       0.89      0.89      0.89    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[88104 11896]\n",
      " [ 9587 90413]]\n",
      "\n",
      "\n",
      "Support Vector Machine\n",
      "\n",
      "Time taken: 13.25853419303894 seconds\n",
      "Training Accuracy:  0.8882466666666666\n",
      "Validation Accuracy:  0.88548\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.89      0.89    100000\n",
      "    Negative       0.89      0.88      0.89    100000\n",
      "\n",
      "    accuracy                           0.89    200000\n",
      "   macro avg       0.89      0.89      0.89    200000\n",
      "weighted avg       0.89      0.89      0.89    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[88688 11312]\n",
      " [11592 88408]]\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "\n",
      "Time taken: 0.4452066421508789 seconds\n",
      "Training Accuracy:  0.850815\n",
      "Validation Accuracy:  0.85055\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.84      0.86      0.85    100000\n",
      "    Negative       0.86      0.84      0.85    100000\n",
      "\n",
      "    accuracy                           0.85    200000\n",
      "   macro avg       0.85      0.85      0.85    200000\n",
      "weighted avg       0.85      0.85      0.85    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[85981 14019]\n",
      " [15871 84129]]\n",
      "\n",
      "\n",
      "Multilayer Perceptron\n",
      "\n",
      "18750/18750 [==============================] - 30s 2ms/step\n",
      "6250/6250 [==============================] - 10s 2ms/step\n",
      "Time taken: 550.6635382175446 seconds\n",
      "Training Accuracy:  0.999095\n",
      "Validation Accuracy:  0.9436\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.94      0.95      0.94    100000\n",
      "    Negative       0.95      0.93      0.94    100000\n",
      "\n",
      "    accuracy                           0.94    200000\n",
      "   macro avg       0.94      0.94      0.94    200000\n",
      "weighted avg       0.94      0.94      0.94    200000\n",
      "\n",
      "Confusion Matrix\n",
      "[[95267  4733]\n",
      " [ 6547 93453]]\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "max_feature_sizes = [500, 1500, 2500]\n",
    "for size in max_feature_sizes:\n",
    "  print(\"Count Vectorizer No. of Features = \", size)\n",
    "  print(\"\")\n",
    "  X_train_scaled, X_val_scaled, X_test_scaled = getCountVectorizerFeatures(size)\n",
    "  print(\"Logistic Regression\")\n",
    "  print(\"\")\n",
    "  LogisticRegressionModel(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Support Vector Machine\")\n",
    "  print(\"\")\n",
    "  SVMClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Multinomial Naive Bayes\")\n",
    "  print(\"\")\n",
    "  MNBClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"\")\n",
    "  print(\"Multilayer Perceptron\")\n",
    "  print(\"\")\n",
    "  MLPClassifier(X_train_scaled, X_val_scaled)\n",
    "  print(\"\")\n",
    "  print(\"-----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd53300-1264-4dd4-a901-0f5a09620eb1",
   "metadata": {
    "id": "efd53300-1264-4dd4-a901-0f5a09620eb1"
   },
   "outputs": [],
   "source": [
    "# # Get a random sample of 100 rows with sentiment equal to Positive\n",
    "# random_sample_sentiment_pos = df[df['sentiment'] == 1].sample(n=50000, random_state=42)\n",
    "\n",
    "# # Get a random sample of 100 rows with sentiment equal to Negative\n",
    "# random_sample_sentiment_neg = df[df['sentiment'] == -1].sample(n=50000, random_state=42)\n",
    "\n",
    "# random_samples_combined = pd.concat([random_sample_sentiment_pos, random_sample_sentiment_neg])\n",
    "\n",
    "# X = random_samples_combined.drop(columns=['sentiment'], axis=1)\n",
    "# y = random_samples_combined['sentiment']\n",
    "\n",
    "# y = y.replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16712dbc-ba46-48ef-b3c3-40ed456ad14b",
   "metadata": {
    "id": "16712dbc-ba46-48ef-b3c3-40ed456ad14b"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Splitting the data into training, validation and testing sets (60%, 20%, 20%)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X['preprocessed_text'], y, test_size=0.4, random_state=42, stratify=y)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "\n",
    "# # Print the shapes of the resulting sets\n",
    "# print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "# print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "# print(\"Testing set shape:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f5ab9e-596a-4c67-b90c-ecc6818c9a27",
   "metadata": {
    "id": "c4f5ab9e-596a-4c67-b90c-ecc6818c9a27"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_file = '../HW2/glove.6B/glove.6B.100d.txt'\n",
    "glove_model = load_glove_embeddings(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5a637ff-1f9c-484b-aec9-399a0a1cf586",
   "metadata": {
    "id": "b5a637ff-1f9c-484b-aec9-399a0a1cf586"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X_train['preprocessed_summary'])\n",
    "X_train_seq = word_tokenizer.texts_to_sequences(X_train['preprocessed_summary'])\n",
    "X_val_seq = word_tokenizer.texts_to_sequences(X_val['preprocessed_summary'])\n",
    "X_test_seq = word_tokenizer.texts_to_sequences(X_test['preprocessed_summary'])\n",
    "\n",
    "# Padding all reviews to fixed length\n",
    "maxlen = 500\n",
    "X_train_padded = pad_sequences(X_train_seq, padding='post', maxlen=maxlen)\n",
    "X_val_padded = pad_sequences(X_val_seq, padding='post', maxlen=maxlen)\n",
    "X_test_padded = pad_sequences(X_test_seq, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ddf13f8-ab40-4f50-9917-2ca2eb0306f1",
   "metadata": {
    "id": "5ddf13f8-ab40-4f50-9917-2ca2eb0306f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17047, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import zeros\n",
    "from numpy import asarray\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "embedding_matrix = zeros((vocab_length, 100))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = glove_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Print Embedding Matrix shape\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e600c637-30e5-45a9-b2e5-a126f421e007",
   "metadata": {
    "id": "e600c637-30e5-45a9-b2e5-a126f421e007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m 337/1875\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:07\u001b[0m 278ms/step - acc: 0.4951 - loss: 0.6934"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lstm_model\n\u001b[1;32m     20\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m getModel()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mlstm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:323\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 323\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    325\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    326\u001b[0m     )\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.0/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "embedding_layer = Embedding(input_dim=vocab_length, output_dim=100, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False)\n",
    "\n",
    "def getModel():\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(embedding_layer)\n",
    "    lstm_model.add(LSTM(128))\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    lstm_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])\n",
    "    return lstm_model\n",
    "\n",
    "lstm_model = getModel()\n",
    "lstm_model.fit(X_train_padded, y_train, epochs=20, batch_size=32, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d900ee-75f8-4a2f-ab2f-50bc8e373b8a",
   "metadata": {
    "id": "99d900ee-75f8-4a2f-ab2f-50bc8e373b8a"
   },
   "outputs": [],
   "source": [
    "predicted_train = lstm_model.predict(X_train)\n",
    "predicted_val = lstm_model.predict(X_val)\n",
    "\n",
    "predicted_train = (predicted_train > 0.5).astype(int)\n",
    "predicted_val = (predicted_val > 0.5).astype(int)\n",
    "\n",
    "accuracy_train = accuracy_score(y_train,predicted_train)\n",
    "\n",
    "accuracy_val = accuracy_score(y_val,predicted_val)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
    "\n",
    "print(\"Training Accuracy: \", accuracy_train)\n",
    "print(\"Validation Accuracy: \", accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d569eb-98d6-4b1a-b7c0-24eea8857335",
   "metadata": {
    "id": "d2d569eb-98d6-4b1a-b7c0-24eea8857335"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
